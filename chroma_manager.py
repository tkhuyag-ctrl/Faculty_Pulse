import chromadb
import json
import uuid
import logging
from typing import List, Dict, Optional, Literal, Union
from chromadb.config import Settings
from enum import Enum
from datetime import datetime

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'chroma_db_operations_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


class ContentType(Enum):
    """Enum for valid content types"""
    AWARD = "Award"
    PUBLICATION = "Publication"
    TALK = "Talk"


class ChromaDBManager:
    """Manager class for ChromaDB operations"""

    def __init__(self, persist_directory: str = "./chroma_db", collection_name: str = "faculty_pulse"):
        """
        Initialize ChromaDB client and collection

        Args:
            persist_directory: Directory to persist the database
            collection_name: Name of the collection to use
        """
        logger.info(f"Initializing ChromaDBManager (persist_dir={persist_directory}, collection={collection_name})")
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection_name = collection_name
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        # Get initial count
        initial_count = self.collection.count()
        logger.info(f"ChromaDB initialized. Collection '{collection_name}' has {initial_count} documents")

    def add_documents(self, documents: List[str], metadatas: List[Dict], ids: Optional[List[str]] = None):
        """
        Add documents to ChromaDB collection

        Args:
            documents: List of document texts
            metadatas: List of metadata dictionaries
            ids: Optional list of unique document IDs. If None, UUIDs will be autogenerated
        """
        logger.info(f"Adding {len(documents)} documents to collection '{self.collection_name}'")

        # Log document details
        for i, (doc, meta) in enumerate(zip(documents[:3], metadatas[:3]), 1):  # Log first 3
            logger.debug(f"  Doc {i}: faculty={meta.get('faculty_name')}, type={meta.get('content_type')}, length={len(doc)} chars")

        # Generate UUIDs if no IDs provided
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in range(len(documents))]
            logger.debug(f"Generated {len(ids)} UUIDs for documents")

        try:
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            logger.info(f"✓ Successfully added {len(documents)} documents to collection '{self.collection_name}'")
            logger.info(f"  Collection now has {self.collection.count()} total documents")
        except Exception as e:
            logger.error(f"✗ Failed to add documents to collection: {str(e)}", exc_info=True)
            raise

    def add_submission_from_json(self, json_file_path: str):
        """
        Add a single submission from a JSON file

        Args:
            json_file_path: Path to JSON file with a single submission

        Expected JSON format:
        {
            "id": "sub_001",  # Optional - if omitted, Chroma will autogenerate a UUID
            "document": "Document content string",
            "metadata": {
                "faculty_name": "Dr. John Doe",
                "date_published": "2026-01-13T10:30:00Z",
                "content_type": "Award",
                "department": "Computer Science"
            }
        }
        """
        with open(json_file_path, 'r', encoding='utf-8') as f:
            submission = json.load(f)

        # Extract data from JSON
        document = submission['document']
        metadata = submission['metadata']
        submission_id = submission.get('id')

        # Call add_single_submission to handle the actual addition
        self.add_single_submission(
            document=document,
            faculty_name=metadata['faculty_name'],
            date_published=metadata['date_published'],
            content_type=metadata['content_type'],
            department=metadata['department'],
            submission_id=submission_id
        )

    def add_single_submission(
        self,
        document: str,
        faculty_name: str,
        date_published: str,
        content_type: str,
        department: str,
        submission_id: Optional[str] = None
    ):
        """
        Add a single submission to the collection

        Args:
            document: Document text content
            faculty_name: Name of the faculty member
            date_published: Publication date in ISO 8601 format (e.g., "2026-01-13T10:30:00Z")
            content_type: Type of content - must be "Award", "Publication", or "Talk"
            department: Department name
            submission_id: Optional unique submission ID. If None, a UUID will be autogenerated

        Raises:
            ValueError: If content_type is not valid
        """
        logger.info(f"Adding submission: faculty={faculty_name}, type={content_type}, dept={department}")
        logger.debug(f"  Document length: {len(document)} characters")
        logger.debug(f"  Date published: {date_published}")

        # Validate content_type
        valid_content_types = {ct.value for ct in ContentType}
        if content_type not in valid_content_types:
            error_msg = f"Invalid content_type '{content_type}'. Must be one of: {valid_content_types}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        metadata = {
            "faculty_name": faculty_name,
            "date_published": date_published,
            "content_type": content_type,
            "department": department
        }

        # Generate UUID if no ID provided
        if submission_id is None:
            submission_id = str(uuid.uuid4())
            logger.debug(f"Generated UUID: {submission_id}")
        else:
            logger.debug(f"Using provided ID: {submission_id}")

        try:
            self.collection.add(
                documents=[document],
                metadatas=[metadata],
                ids=[submission_id]
            )
            logger.info(f"✓ Successfully added submission '{submission_id}' for {faculty_name}")
            logger.info(f"  Collection now has {self.collection.count()} total documents")
        except Exception as e:
            logger.error(f"✗ Failed to add submission '{submission_id}': {str(e)}", exc_info=True)
            raise

    def query_submissions(self, query_text: str, n_results: int = 5,
                         content_type: Optional[str] = None,
                         department: Optional[str] = None,
                         year_filter: Optional[Union[str, List[str]]] = None,
                         date_range: Optional[Dict[str, str]] = None):
        """
        Query submissions from the collection with semantic and metadata filtering

        Args:
            query_text: Query text for semantic search
            n_results: Number of results to return
            content_type: Optional filter by content type (Award, Publication, Talk)
            department: Optional filter by department
            year_filter: Optional filter by year - single year string or list of years (e.g., "2024", ["2024", "2025"])
            date_range: Optional date range filter with 'start' and/or 'end' keys
                       Format: {"start": "2023-01-01", "end": "2024-12-31"}

        Returns:
            Query results
        """
        where_filter = None
        where_document_filter = None

        # Build metadata filters - ChromaDB requires $and for multiple conditions
        filters = []

        # Add content type filter
        if content_type:
            filters.append({"content_type": {"$eq": content_type}})

        # Add department filter
        if department:
            filters.append({"department": {"$eq": department}})

        # Combine filters using $and if multiple
        if len(filters) > 1:
            where_filter = {"$and": filters}
        elif len(filters) == 1:
            where_filter = filters[0]

        # Year filter will be applied post-query as ChromaDB filtering is complex
        # We'll fetch more results and filter them in Python

        # Add date range filter using $gte and $lte operators
        if date_range:
            if "start" in date_range or "end" in date_range:
                date_filter = {}
                if "start" in date_range:
                    date_filter["$gte"] = date_range["start"]
                if "end" in date_range:
                    date_filter["$lte"] = date_range["end"]
                date_range_filter = {"date_published": date_filter}

                # Combine with existing filters
                if where_filter:
                    where_filter = {"$and": [where_filter, date_range_filter]}
                else:
                    where_filter = date_range_filter

        # If we have a year filter, fetch many more results so we can filter them post-query
        # Fetch up to 10x the requested amount to ensure we get enough after filtering
        fetch_count = n_results * 10 if year_filter else n_results

        query_kwargs = {
            "query_texts": [query_text],
            "n_results": fetch_count
        }

        # Add metadata filters if any
        if where_filter:
            query_kwargs["where"] = where_filter

        try:
            results = self.collection.query(**query_kwargs)

            # Apply year filtering post-query if needed
            if year_filter and results and 'metadatas' in results:
                filtered_results = {
                    'ids': [[]],
                    'distances': [[]],
                    'metadatas': [[]],
                    'documents': [[]]
                }

                skipped_count = 0
                examined_count = 0

                for i, metadata in enumerate(results['metadatas'][0]):
                    date_pub = metadata.get('date_published', '')
                    examined_count += 1

                    # Check if date matches year filter
                    should_include = False
                    if isinstance(year_filter, list):
                        # Check if date starts with any of the years in the list
                        # Date format is YYYY-MM-DD, so we check first 4 characters
                        if date_pub and len(date_pub) >= 4:
                            year_from_date = date_pub[:4]
                            should_include = year_from_date in year_filter
                    elif isinstance(year_filter, str):
                        # Single year filter
                        if date_pub and len(date_pub) >= 4:
                            year_from_date = date_pub[:4]
                            should_include = year_from_date == year_filter

                    if should_include:
                        filtered_results['ids'][0].append(results['ids'][0][i])
                        filtered_results['distances'][0].append(results['distances'][0][i])
                        filtered_results['metadatas'][0].append(results['metadatas'][0][i])
                        filtered_results['documents'][0].append(results['documents'][0][i])

                        # Stop once we have enough results
                        if len(filtered_results['ids'][0]) >= n_results:
                            break
                    else:
                        skipped_count += 1

                logger.info(f"Year filter applied: examined {examined_count} results, kept {len(filtered_results['ids'][0])}, skipped {skipped_count}")

                # If we didn't get enough results, warn about it
                if len(filtered_results['ids'][0]) < n_results:
                    logger.warning(f"Only found {len(filtered_results['ids'][0])} results matching year filter {year_filter} (requested {n_results})")

                # Sort filtered results by date (most recent first) while maintaining semantic relevance
                # We'll sort by date within the filtered results to prioritize newer content
                if len(filtered_results['ids'][0]) > 0:
                    # Create tuples of (date, distance, index) for sorting
                    results_with_dates = []
                    for idx in range(len(filtered_results['ids'][0])):
                        date_str = filtered_results['metadatas'][0][idx].get('date_published', '')
                        distance = filtered_results['distances'][0][idx]
                        results_with_dates.append((date_str, distance, idx))

                    # Sort by date (descending, most recent first), then by distance (ascending, most similar first)
                    results_with_dates.sort(key=lambda x: (x[0], -x[1]), reverse=True)

                    # Reorder filtered results based on sorted indices
                    sorted_filtered = {
                        'ids': [[]],
                        'distances': [[]],
                        'metadatas': [[]],
                        'documents': [[]]
                    }

                    for _, _, idx in results_with_dates:
                        sorted_filtered['ids'][0].append(filtered_results['ids'][0][idx])
                        sorted_filtered['distances'][0].append(filtered_results['distances'][0][idx])
                        sorted_filtered['metadatas'][0].append(filtered_results['metadatas'][0][idx])
                        sorted_filtered['documents'][0].append(filtered_results['documents'][0][idx])

                    return sorted_filtered

                return filtered_results

            return results
        except Exception as e:
            logger.error(f"Query failed: {str(e)}")
            # Fall back to simpler query without date filters
            query_kwargs = {
                "query_texts": [query_text],
                "n_results": n_results
            }
            if content_type or department:
                simple_filter = {}
                if content_type:
                    simple_filter["content_type"] = content_type
                if department:
                    simple_filter["department"] = department
                query_kwargs["where"] = simple_filter
            results = self.collection.query(**query_kwargs)
            return results

    def get_collection_count(self):
        """Get the number of documents in the collection"""
        return self.collection.count()

    def get_all_submissions(self):
        """
        Get all submissions from the collection

        Returns:
            Dictionary containing all documents, metadata, and IDs
        """
        count = self.collection.count()
        if count == 0:
            return {"ids": [], "documents": [], "metadatas": []}

        # Get all documents from the collection
        results = self.collection.get()
        return results

    def display_all_submissions(self):
        """
        Display all submissions in a readable format
        """
        results = self.get_all_submissions()

        if len(results['ids']) == 0:
            print("No submissions found in the database.")
            return

        print(f"\n{'='*80}")
        print(f"ALL SUBMISSIONS IN DATABASE (Total: {len(results['ids'])})")
        print(f"{'='*80}\n")

        for i, (doc_id, doc, metadata) in enumerate(zip(
            results['ids'],
            results['documents'],
            results['metadatas']
        ), 1):
            print(f"{i}. ID: {doc_id}")
            print(f"   Faculty: {metadata['faculty_name']}")
            print(f"   Department: {metadata['department']}")
            print(f"   Type: {metadata['content_type']}")
            print(f"   Date: {metadata['date_published']}")
            print(f"   Document: {doc}")
            print(f"   {'-'*76}\n")

    def delete_submission(self, submission_id: str):
        """Delete a submission by ID"""
        logger.info(f"Deleting submission: {submission_id}")
        try:
            self.collection.delete(ids=[submission_id])
            logger.info(f"✓ Successfully deleted submission '{submission_id}'")
            logger.info(f"  Collection now has {self.collection.count()} total documents")
        except Exception as e:
            logger.error(f"✗ Failed to delete submission '{submission_id}': {str(e)}", exc_info=True)
            raise

    def clear_database(self):
        """
        Clear all submissions from the database

        Warning: This will permanently delete all data in the collection
        """
        count = self.collection.count()
        logger.warning(f"Clearing database with {count} documents")

        if count == 0:
            logger.info("Database is already empty. Nothing to clear.")
            print("Database is already empty.")
            return

        try:
            # Delete the collection and recreate it
            logger.info(f"Deleting collection '{self.collection_name}'...")
            self.client.delete_collection(name=self.collection_name)

            logger.info(f"Recreating collection '{self.collection_name}'...")
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )

            logger.info(f"✓ Database cleared successfully. Deleted {count} submission(s).")
            print(f"✓ Database cleared. Deleted {count} submission(s).")
        except Exception as e:
            logger.error(f"✗ Failed to clear database: {str(e)}", exc_info=True)
            raise

    def update_submission(
        self,
        submission_id: str,
        document: str,
        faculty_name: str,
        date_published: str,
        content_type: str,
        department: str
    ):
        """
        Update an existing submission

        Args:
            submission_id: Unique submission ID
            document: Document text content
            faculty_name: Name of the faculty member
            date_published: Publication date in ISO 8601 format
            content_type: Type of content - must be "Award", "Publication", or "Talk"
            department: Department name

        Raises:
            ValueError: If content_type is not valid
        """
        logger.info(f"Updating submission: {submission_id}")
        logger.debug(f"  New faculty: {faculty_name}, type: {content_type}, dept: {department}")
        logger.debug(f"  New document length: {len(document)} characters")

        # Validate content_type
        valid_content_types = {ct.value for ct in ContentType}
        if content_type not in valid_content_types:
            error_msg = f"Invalid content_type '{content_type}'. Must be one of: {valid_content_types}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        metadata = {
            "faculty_name": faculty_name,
            "date_published": date_published,
            "content_type": content_type,
            "department": department
        }

        try:
            self.collection.update(
                ids=[submission_id],
                documents=[document],
                metadatas=[metadata]
            )
            logger.info(f"✓ Successfully updated submission '{submission_id}'")
        except Exception as e:
            logger.error(f"✗ Failed to update submission '{submission_id}': {str(e)}", exc_info=True)
            raise


# Example usage
if __name__ == "__main__":
    """
    Simple example demonstrating basic usage.
    For comprehensive tests, run: python test_chroma_manager.py
    """
    print("ChromaDB Manager - Quick Example")
    print("-" * 40)

    # Initialize the manager
    manager = ChromaDBManager()

    # Add a submission with autogenerated ID
    print("\nAdding a submission...")
    manager.add_single_submission(
        document="Dr. Johnson received the Outstanding Research Award for contributions to machine learning.",
        faculty_name="Dr. Alice Johnson",
        date_published="2026-01-13T10:00:00Z",
        content_type="Award",
        department="Computer Science"
    )

    # Display all submissions
    print("\nAll submissions in database:")
    manager.display_all_submissions()

    print("\nFor comprehensive tests, run: python test_chroma_manager.py")
